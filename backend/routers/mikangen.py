from dotenv import load_dotenv
import os   

from typing import List, TypedDict

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from google.cloud import storage
import re
from langchain_mongodb import MongoDBAtlasVectorSearch
from pymongo import MongoClient
from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langgraph.graph import END, StateGraph, START

from fastapi import APIRouter, HTTPException, Depends
import json

from .mikangen_helpers import generate_title, format_text_to_google_doc

from sqlalchemy.orm import Session
from database import get_db
from .autostart import upload_autostart_url
from .ai_attachments import upload_ai_attachment_url
from model import AutostartIn, AIAttachmentIn, MiKanGenRequest

load_dotenv()
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')

##########################################################################################################################
# State
##########################################################################################################################

class ReWOO(TypedDict):
    # The initial task description
    task: str

    # The big-chunk plan string generated by the planner
    plan_string: str

    # The step-by-step plan extracted from plan_string
    steps: List[str]

    # Intermediate results after tool calls
    results: dict

    result: str # equivalent to autostart

    # A list of file paths retrieved by the search tool
    retrieved_docs: List[str]

    # The content generated by the autostart tool
    autostart_content: str


##########################################################################################################################
# Node: Planner
##########################################################################################################################

planner_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY)

system_prompt = """
You are MiKan, an advanced AI-powered productivity platform designed to revolutionize team workflows from meeting discussions to fully prepared, actionable tasks. Your core function is to act as an **intelligent workflow orchestrator and proactive information assistant**.

Your capabilities include:

1.  **Intelligent Query Generation for RAG:** For each identified task, you will proactively determine *all* the information and resources the assignee will need to immediately begin and successfully complete that task, even if not explicitly requested in the meeting.
    * You will generate highly relevant, specific search queries for retrieval from the internal vector database (knowledge base).
    * Prioritize explicit mentions (e.g., 'Product X brochure'), but also infer implicit needs based on task type, entities (e.g., 'new client in Germany' -> 'GDPR compliance'), and common pre-work steps for similar tasks.
2.  **Proactive Information Retrieval & Synthesis ('Auto-Start'):**
    * Utilize the generated queries to retrieve the most relevant documents, data snippets, and insights from the internal vector database.
    * Synthesize this retrieved information into a concise, actionable format.
    * This synthesized content should serve as the 'pre-start' package, directly embedded within the task for the assignee. This must include not only directly requested documents but also:
        * Relevant policies, procedures, or compliance guidelines.
        * Key insights or summaries from related reports.
        * Templates or checklists that streamline the task.
        * Contact information for relevant internal stakeholders or experts.
    * Aim to provide information that mitigates 'cognitive blind spots' - knowledge the user might not even realize they need.

**Persona & Principles:**

* **Proactive & Anticipatory:** Always aim to provide information *before* it's asked for.
* **Accurate & Contextual:** Ensure all extracted and retrieved information is highly relevant and precise to the task's specific context.
* **Concise & Actionable:** Present information clearly, avoiding unnecessary verbosity, and making it directly usable for the task.
* **Reliable & Trustworthy:** Strive for consistent, high-quality output that users can depend on.
* **Security & Privacy Aware:** Understand that sensitive information may be handled; never generate or expose unauthorized data. (This is a high-level instruction; actual security is in your backend.)

**Your ultimate goal is to eliminate 'pre-work friction' and ensure that every task generated from a meeting can be 'auto-started' efficiently and comprehensively.**
"""

user_prompt = """
For the following task, make detailed plans that can solve the problem step by step. For each plan, indicate which external tool
together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools.
(Plan, #E1, Plan, #E2, Plan, ...)

Tools can be one of the following:
(1) RAG[input]: A retrieval tool that searches for relevant content from a MongoDB Vector Store, which acts as the internal knowledge base.
The knowledge base contains the following relevant documents:
{files_list}

Use this when you need information that is not publicly available and must be retrieved from proprietary or internal documents, such as
company reports, meeting transcripts, technical documentation, or internal guidelines. The input should be a natural language search query
that clearly expresses the information you are looking for.

---

Before selecting a tool, if the user query is complex or contains multiple sub-questions, **decompose it into simpler, independent sub-queries**.
Each sub-query should focus on retrieving one specific piece of information. For example:

Input: "Who is the CEO of Company X, and what product did they launch in 2023?"
â†’ Sub-queries:
1. "Who is the CEO of Company X?"
2. "What product did Company X launch in 2023?"

Then apply RAG[input] individually for each sub-query.

If decomposition is not necessary (i.e., the query is already simple), proceed directly with the appropriate tool.

Only use internal tools like RAG if the required information is likely stored in the internal documents listed above.

(2) LLM[input]: A pretrained LLM like yourself. Useful when you need to act with general
world knowledge and common sense. Prioritize it when you are confident in solving the problem
yourself. Input can be any instruction.

For example,
Task: Thomas, Toby, and Rebecca worked a total of 157 hours in one week. Thomas worked x
hours. Toby worked 10 hours less than twice what Thomas worked, and Rebecca worked 8 hours
less than Toby. How many hours did Rebecca work?
Plan: Given Thomas worked x hours, translate the problem into algebraic expressions and solve
with Wolfram Alpha. #E1 = WolframAlpha[Solve x + (2x - 10) + ((2x - 10) - 8) = 157]
Plan: Find out the number of hours Thomas worked. #E2 = LLM[What is x, given #E1]
Plan: Calculate the number of hours Rebecca worked. #E3 = Calculator[(2 * #E2 - 10) - 8]

Begin!
Describe your plans with rich details. Each Plan should be followed by only one #E.

Task: {task}"""

planner_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            system_prompt
        ),
        (
            "human",
            user_prompt
        )
    ]
)

planner_chain = planner_prompt | planner_llm


#######################################################
# Helper Function in Planner Node
#######################################################

def list_gcs_files(project_name: str, bucket_name: str, prefix: str = "") -> list[str]:
    """Lists all files (objects) in a GCS bucket with an optional prefix."""
    client = storage.Client(project=project_name)
    bucket = client.get_bucket(bucket_name)

    file_names = []
    blobs = bucket.list_blobs(prefix=prefix)
    for blob in blobs:
        # GCS "directories" are often represented by objects ending with a '/'
        # You can choose to skip these if you only want actual files.
        if blob.name.endswith("/"):
            continue
        file_names.append(blob.name)
    return file_names

GCP_PROJECT_NAME = "mikan01"
GCS_BUCKET_NAME = "mikan-rag-source"
GCS_PREFIX = "" # Optional: set a prefix if you want to list files in a specific "folder"
files_list = list_gcs_files(GCP_PROJECT_NAME, GCS_BUCKET_NAME, GCS_PREFIX)

# Regex to match expressions of the form E#... = ...[...]
regex_pattern = r"Plan:\s*(.+)\s*(#E\d+)\s*=\s*(\w+)\s*\[([^\]]+)\]"

def get_plan(state: ReWOO):
    task = state["task"]
    result = planner_chain.invoke({"task": task, "files_list": files_list})
    # Find all matches in the sample text
    matches = re.findall(regex_pattern, result.content)
    return {"steps": matches, "plan_string": result.content}


##########################################################################################################################
# Tool: RAG
##########################################################################################################################

rag_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", api_key=GEMINI_API_KEY)
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001", google_api_key=GEMINI_API_KEY)

client = MongoClient(os.getenv("MONGODB_ATLAS_CLUSTER_URI"))

DB_NAME = "mikan_db"
COLLECTION_NAME = "logistic_team_collection"
ATLAS_VECTOR_SEARCH_INDEX_NAME = "test-index-1"

MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]

vector_store = MongoDBAtlasVectorSearch(
    collection=MONGODB_COLLECTION,
    embedding=embeddings,
    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
    relevance_score_fn="cosine",
)

retriever = vector_store.as_retriever(
    search_type = "similarity_score_threshold",
    search_kwargs = {
        "k": 8,
        'score_threshold': 0.6
    }
)

file_retriever = vector_store.as_retriever(
    search_type = "similarity_score_threshold",
    search_kwargs = {
        'score_threshold': 0.85
    }
)

rag_system_prompt = (
    "Use the given context to answer the question. "
    "If you don't know the answer, say you don't know. "
    "Context: {context}"
)

rag_prompt = ChatPromptTemplate.from_messages([
    ("system", rag_system_prompt),
    ("human", "{input}")
])

document_chain = create_stuff_documents_chain(rag_llm, rag_prompt)
rag_chain = create_retrieval_chain(retriever, document_chain)
rag_file_chain = create_retrieval_chain(file_retriever, document_chain)


##########################################################################################################################
# Node: Tool Manager
##########################################################################################################################

worker_llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", google_api_key=GEMINI_API_KEY)

def _get_current_task(state: ReWOO):
    if "results" not in state or state["results"] is None:
        return 1
    if len(state["results"]) == len(state["steps"]):
        return None
    else:
        return len(state["results"]) + 1

def tool_execution(state: ReWOO):
    """Worker node that executes the tools of a given plan."""
    _step = _get_current_task(state)
    _, step_name, tool, tool_input = state["steps"][_step - 1]
    _results = (state["results"] or {}) if "results" in state else {}
    _files = (state["retrieved_docs"] or []) if "retrieved_docs" in state else []
    for k, v in _results.items():
        tool_input = tool_input.replace(k, v)
    if tool == "RAG":
        print("Running RAG...")
        result = rag_chain.invoke({"input": tool_input})
        file_result = rag_file_chain.invoke({"input": tool_input})
        
        list_of_references = file_result["context"]
        sources = [doc.metadata['source'] for doc in list_of_references]
        _files.extend(sources)
    elif tool == "LLM":
        print("Running LLM...")
        result = worker_llm.invoke(tool_input)
    else:
        raise ValueError
    _results[step_name] = str(result)

    return {"results": _results, "retrieved_docs": _files}


##########################################################################################################################
# Node: Solver
##########################################################################################################################

solver_llm = ChatGoogleGenerativeAI(model="gemini-2.5-pro", google_api_key=GEMINI_API_KEY)

solve_prompt = """Solve the following task or problem. To solve the problem, we have made step-by-step Plan and \
retrieved corresponding Evidence to each Plan. Use them with caution since long evidence might \
contain irrelevant information.

{plan}

Now solve the question or task according to provided Evidence above. Respond with the answer
directly with no extra words.

Task: {task}
Response:"""

solver_prompt = ChatPromptTemplate.from_messages([("user", solve_prompt)])

solver_chain = solver_prompt | solver_llm

def solve(state: ReWOO):
    plan = ""
    for _plan, step_name, tool, tool_input in state["steps"]:
        _results = (state["results"] or {}) if "results" in state else {}
        for k, v in _results.items():
            tool_input = tool_input.replace(k, v)
            step_name = step_name.replace(k, v)
        plan += f"Plan: {_plan}\n{step_name} = {tool}[{tool_input}]"

    solver_result = solver_chain.invoke({"plan": plan, "task": state["task"]})

    _files = (state["retrieved_docs"] or []) if "retrieved_docs" in state else []
    return {"result": solver_result.content, "retrieved_docs": _files}


##########################################################################################################################
# Define Graph
##########################################################################################################################

def _route(state):
    _step = _get_current_task(state)
    # return _step is None
    if _step is None:
        # We have executed all tasks
        return "finalise"
    else:
        # We are still executing tasks, loop back to the "tool" node
        return "continue"
    
graph = StateGraph(ReWOO)
graph.add_node("plan", get_plan)
graph.add_node("tools", tool_execution)
graph.add_node("solve", solve)
graph.add_edge("plan", "tools")
graph.add_edge("solve", END)
graph.add_conditional_edges(
    "tools",
    _route,
    {
        "finalise": "solve",
        "continue": "tools",
    }
)
graph.add_edge(START, "plan")

app = graph.compile()


##########################################################################################################################
# Langgraph ends here
##########################################################################################################################


router = APIRouter()
@router.post("/mikangen")
def main_agent(request: MiKanGenRequest, db: Session = Depends(get_db)):
    task = request.task
    task_id = request.task_id
    try:
        for s in app.stream({"task": task}):
            print(json.dumps(s, indent=2))
            print("---")


        # Extract the autostart content and format into Google Doc
        autostart_content = s['solve']['result']
        print("Autostart Content:")
        print(autostart_content)

        autostart_title = generate_title(autostart_content)
        formatted_doc_url = format_text_to_google_doc(
            autostart_content,
            autostart_title,
            folder_id="1wjMWSKk8Z0VToz8hS05VrZXkyVWcP3dc",
        )
        # Save autostart to database
        upload_autostart_url(autostart_data=AutostartIn(
            task_id=task_id,
            title=autostart_title,
            url=formatted_doc_url
        ), 
        db=db)




        # Clean duplicate files
        unique_files_set = set(s['solve']['retrieved_docs'])
        unique_files_list = list(unique_files_set)
        print("Files:")
        print(unique_files_list)

        http_urls = [url.replace("gs://", "https://storage.googleapis.com/") for url in unique_files_list]

        for each_url in http_urls:
            upload_ai_attachment_url(ai_attachment_data=AIAttachmentIn(
                task_id=task_id,
                title=each_url.split("/")[-1],
                url=each_url
            ),
            db=db)

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An error occurred during MiKanGen: {str(e)}")
    

# from model import AutostartIn

# @router.post("/testing")
# def main_test(task: str, task_id: int, db: Session = Depends(get_db)):
#     try:
#         vari = upload_autostart_url(autostart_data=AutostartIn(
#             task_id=41,
#             title="Media Invitation: Shaver Gaming Expo 2025",
#             url="https://docs.google.com/document/d/1ouAlSrG-1J4QXAeiALmi-cmxqgNegRuLWW-7DhhzAgE/edit"
#         ), 
#         db=db)
#         print(vari.task_id)

#     except Exception as e:
#         raise HTTPException(status_code=500, detail=f"An error occurred during testing: {str(e)}")